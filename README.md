# ParliamentMining
Collect, Process, Analyze and Visualize the parliamentary data and news from 5 different countries: USA, UK IL, TN, CA. we collected data from the year 2000.
* Developers: Ayal Swaid, Daniel Hasson, George Kanazi, Jiana Bdarneh

* Menthors: Mr. Shai Berkovitz, Dr. Michael Fire

The following diagram demonstrates the whole system flow:
![image](https://github.com/user-attachments/assets/37091150-5141-49a5-afbc-f9ee5480730c)

# About the repository
There are three seperate directories that are related with each other:
## 1. DataPipeline - Contains the Collecting and processing steps (including saving the data in CSV files according to the schema)
The code is designed according to the below UML diagrame, such that there are collector components for each contry and  Processor components for each table in the schema. The pipe here starts by collecting data (via API or web scraping) and saving cache files in Data/to_process folder, and then the processor reads these files, perform relevant process steps and then save the results in CSV(metadata) and Json files (speeches files).
The whole process starts from main.py which calls CollectorManager and ProcessorManager where they call the specified collectors / processors (that works according to Data/process.json).
* Note: set the range of the start date of the data you want to collect in Data/process.json
Result CSV and json files are found in the Data directory under csv_files and speeches folders.
## 2. DataScience - Contains the Analysis process
There is a jupyter notebook that includes the GPT prompts used for debate / news article classifications, summarization and explainability. It also includes sentiment analysis of the parliamentary debates. Basically it reads the CSV files generated by the processor in the DataPipeline and perform the mentioned ML tasks to be ready to visualize. There are some tasks performed in real-time and some require pre-compute (before deployment)
## 3. Web App - Flask
Here is the code for our web app, it is a simple and small Flask App and Dash app for visualization. We simply visualized the data after the analysis steps, for detailed information about the dashboard and results please checkout our paper. 

# Requirements
1. You need a windows machine to run Israel collector and processor (because it uses microsoft word application from the system to process DOC files).
2. Download Python libraries found in DataPipeline/Data/requirements.txt
3. Network connection
4. Some countries uses selenium webdriver that does not work on headless mode, consider using a machine with GUI support.
5. some data API require API keys to work (USA, Canada, and UK news), you may consider generating your own API key (it is free but limited) 


# Database schema
![image](https://github.com/user-attachments/assets/aa4d5c02-5ede-4ee0-b9ef-9a07b113597c)


# Data Pipeline UML
![UML drawio (1)](https://github.com/AyalSwaid/ParliamentMining/assets/57876635/a6b818af-938e-4204-9101-e4d01fca36a9)
